
# task 3
# Use k-nearest neighbours to classify the iris data set with some different values for k,
# and with uniform and distance-based weights. What will happen when k grows larger
# for the different cases? Why?

# task 4
# Compare the classification models for the iris data set that are generated by knearest neighbours
#  #(for the different settings from question 3) and by logistic
# regression. Calculate confusion matrices for these models and discuss the
# performance of the various models.


import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.metrics import plot_confusion_matrix

# import some data to play with
iris = datasets.load_iris()

X = iris["data"][:, :2]
y = iris["target"]
target_names = iris["target_names"]

h = .02  # step size in the mesh

n_neighbors_array = np.array([1, 5, 15, 25])
distributions = ['uniform', 'distance']

# setup multiple plots within one window
fig, ax = plt.subplots(nrows=len(distributions), ncols=len(
    n_neighbors_array), figsize=(10, 10))
plt.tight_layout()

# indices for determining placement of figure in subplot
col = 0
row = 0

for n_neighbors in n_neighbors_array:
    for weights in distributions:

        # we create an instance of Neighbours Classifier and fit the data.
        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
        clf.fit(X, y)

        # create confusion matrix
        plot_confusion_matrix(
            clf, X, y, display_labels=target_names, ax=ax[row, col],  cmap=plt.get_cmap("Blues"), xticks_rotation=90)
        titleString = "K: " + str(n_neighbors) + \
            "\n Distribution: " + str(weights)
        ax[row, col].set_title(titleString)

        row = row + 1

    col = col + 1
    row = 0

# looks nicer
plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95,
                    top=0.95, hspace=0.1, wspace=0.5)

plt.show()
