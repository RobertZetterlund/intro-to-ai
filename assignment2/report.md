##  *Question 1*

## Task A 
<!-- What are the values of the slope and intercept of the regression line? -->

   * slope: 20337
   * intercept: 2073775


## Task B
<!-- Use this model to predict the selling prices of houses which have living area
222 100m ,150m and200m . -->

Using the slope and the intercept values we create a regression line (`f(x) = kx + m`)

```python
f(x) = 20337*x + 2073775
```
![regression-line-scatter](fig/regression.png)

Using our model we predict prices for houses with the area of 100,150 and 200 m2.

```python
f(100) = 4107500
f(150) = 5124362
f(200) = 6141224
```

## Task C
<!-- Draw a residual plot. -->

![residuals plot](fig/residuals.png)

## Task D 
<!-- Discuss the results, and how the model could be improved. -->
Regarding the results.

*   our R-squared "score" is 0.543,

Improvements
- Adding a datapoint at (0,0) will create a more realistic model. Buying a very small house is very expensive right now. A 0m2 house costs 2 million SEK!
- Not assuming that there is a linear relationship. It might not be the case that the data is linear. 
- More datapoints will give a more precise model.
- Take more factors into account
    * Area of land in measurements
    * Year of building the house
    * Is it newly renovated?
    * Do we have additional living space? (biarea)
    * What is the annual cost of keeping the property?
    * Where is it located? Near city centre or not? Near schoold and public transport?




# Task 2 
<!-- Use a confusion matrix to evaluate the use of logistic regression to classify the iris data set. Use the one-vs-rest option to use the same setup as in the lectures for multiclass regression -->

```python
# Load iris dataset and get data and classification
bunch = load_iris()
X = bunch["data"]
y = bunch["target"]

# Get classnames (setosa, versicolor, virginica)
class_names = bunch["target_names"]

# Train one logistic classifier for each class
clf1 = OneVsRestClassifier(LogisticRegression()).fit(X, y)

# Create confusion matrix
plot_confusion_matrix(clf1, X,y, display_labels=class_names, cmap=plt.get_cmap("Blues"))
```

![single-confusion-matrix](fig/single-confusion-matrix.png)



# Task 3 and 4

Below are an multi matrix plot that shows confusion matrices for different values of k. Blue matrices have distribution set to "uniform", Green ones have it set to "distance". 

![multi-confusion-matrix](fig/multi-confusion-matrix.png)


By looking at the top row of confusion matrices displaying an uniform distribution, we notice that the accuracy decreases through k=1,5,50,100. 

By looking at the bottom row of confusion matrices displaying an distance distribution, we notice that the accuracy is perfect through k=1,5,50,100. 

### To answer the question "What will happen when k grows larger for the different cases? Why?"

Intuitively, we be



<!-- 3... Use k-nearest neighbours to classify the iris data set with some different values for k, and with uniform and distance-based weights. What will happen when k grows larger for the different cases? Why? -->



<!-- 4... Compare the classification models for the iris data set that are generated by k- nearest neighbours (for the different settings from question 3) and by logistic regression. Calculate confusion matrices for these models and discuss the performance of the various models. -->



# Task 5

<!-- Explain why it is important to use a separate test (and sometimes validation) set -->

Overfitting?
