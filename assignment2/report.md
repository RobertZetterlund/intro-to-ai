##  *Question 1*

## Task A 
<!-- What are the values of the slope and intercept of the regression line? -->

   * slope: 20337
   * intercept: 2073775


## Task B
<!-- Use this model to predict the selling prices of houses which have living area
222 100m ,150m and200m . -->

Using the slope and the intercept values we create a regression line (`f(x) = kx + m`)

```python
f(x) = 20337*x + 2073775
```
![regression-line-scatter](fig/regression.png)

Using our model we predict prices for houses with the area of 100,150 and 200 m2.

```python
f(100) = 4107500
f(150) = 5124362
f(200) = 6141224
```

## Task C
<!-- Draw a residual plot. -->

![residuals plot](fig/residuals.png)

## Task D 
<!-- Discuss the results, and how the model could be improved. -->
### Regarding the results.

The model we have created has some issues. According to the model, it is very expensive to buy small houses. For example, 0 m2 costs 2 000 000 kr. Furthermore, in the residual plot one can see that the model often underestimate or overestimate the price of a house by quite a lot. One house is underestimated by 2 000 000 kr. 

The coefficient of determination of the predictions is around 0.53. This basically means that 53 % of the variation in y can be explained by the x-variables or in other words, 53 % of the prices can be predicted by the area of the house. Becuase of this, our model does not seem to be very reliable.



### Improvements
- Adding a datapoint at (0,0) will create a more realistic model. 
- Adding more datapoints will give a more precise model.
- Take more factors into account
    * Area of land in measurements
    * Year of building the house
    * Is it newly renovated?
    * Do we have additional living space? (biarea)
    * What is the annual cost of keeping the property?
    * Where is it located? Near city centre or not? Near schoold and public transport?




# Task 2 
<!-- Use a confusion matrix to evaluate the use of logistic regression to classify the iris data set. Use the one-vs-rest option to use the same setup as in the lectures for multiclass regression -->
The below code snippet shows how the confusion matrix below was caluclated. 

```python
# Load iris dataset and get data and classification
bunch = load_iris()
X = bunch["data"]
y = bunch["target"]

# Get classnames (setosa, versicolor, virginica)
class_names = bunch["target_names"]

# divide into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Train one logistic classifier for each class
clf1 = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)

# Create confusion matrix
plot_confusion_matrix(clf1, X_test,y_test, display_labels=class_names, cmap=plt.get_cmap("Blues"))
```

![single-confusion-matrix](fig/single-confusion-matrix.png)

# Task 3 and 4

Below are an multi matrix plot that shows confusion matrices for different values of k. Blue matrices have distribution set to "uniform", Green ones have it set to "distance". 

![multi-confusion-matrix](fig/multi-confusion-matrix.png)


By looking at the top row of confusion matrices displaying an uniform distribution, we notice that the accuracy decreases through k=1,5,50,100. 

By looking at the bottom row of confusion matrices displaying an distance distribution, we notice that the accuracy is perfect through k=1,5,50,100. 

### To answer the question "What will happen when k grows larger for the different cases? Why?"

We note that we have alotted 33% of the datapoints to testing, totalt of 50 being: 19 setosa, 15 versicolor and 16 virginica.
This means that we have *trained* our model on 100 datapoints: 31 setotas, 35 versicolor and 34 virginica. 

We start of by inspecting the results of k=100 with uniform distribution. Here, all predictions are versicolor, which has a reasonable explaination. Recall the distribution in the training set, we have a majority of virginica. 
If any given testing point were to evaluate their 100 nearest neigbors, their distribution would be

| setosas  | versicolor  | virginca  |
|---|---|---|
|   |   |   |




<!-- 3... Use k-nearest neighbours to classify the iris data set with some different values for k, and with uniform and distance-based weights. What will happen when k grows larger for the different cases? Why? -->



<!-- 4... Compare the classification models for the iris data set that are generated by k- nearest neighbours (for the different settings from question 3) and by logistic regression. Calculate confusion matrices for these models and discuss the performance of the various models. -->



# Task 5

 The reason it is important to use a seperate test set is because you want to be able to check how well your model is performing. If you don't split the data into training and test data, but instead use all data for both training and testing, your model won't generalize well for unseen data. Your model will always do well on the "test" data as it is the same data that has been used for the training, but when tasked to predict unseen data, the results probably won't be as good. 

 If you at the other hand do split the data into training and test data, you will be able to compare different models and see which does best on the unseen test data. If you then a find a model that performs well on the unseen test data, you can be more confident that it will predict any new unseen data quite well.
<!-- Explain why it is important to use a separate test (and sometimes validation) set -->

Overfitting?
