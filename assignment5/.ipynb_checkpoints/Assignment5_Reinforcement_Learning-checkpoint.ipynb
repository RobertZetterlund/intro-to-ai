{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DAT405 Introduction to Data Science and AI, 2010-2021, Study Period 1** <br/>\n",
    "**Assignment 5: Reinforcement learning and Classification** <br/>\n",
    "**Due Date: Oct 6, 23:59** <br/>\n",
    "\n",
    "**Robert Zetterlund: 20hrs**\n",
    "\n",
    "**Tobias Lindroth: 20hrs**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**What to submit**\n",
    "*   **The entire assignment should be submitted through the notebook. No separate file will be accepted.**<br/>\n",
    "\n",
    "*In the notebook:*\n",
    "*\tState your names and how many hours each person spent on the assignment.\n",
    "*\tThe solutions and answers to the theoretical and practical problems, including LaTeX math-mode equations, plots and tables etc.\n",
    "*\tAll plots/results should be visible such that the notebook does not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.<br/>\n",
    "\n",
    "*Before submitting:*\n",
    "*   Make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-check**<br/>\n",
    "Is all the required information included? Have you answered all questions to the best of your ability? Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?) Does your notebook run and can reproduce the results, plots and tables?\n",
    "\n",
    "Do not submit an incomplete assignment! We are available to help you, and you\n",
    "can receive a short extension if you contact us.\n",
    "\n",
    "**Grading**<br/>\n",
    "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
    "*\tPresent clear arguments\n",
    "*\tPresent the results in a pedagogical way\n",
    "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
    "*\tGive correct solutions\n",
    "*\tMake sure that the code is well commented \n",
    "\n",
    "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (Assignment5_Reinforcement_Learning.ipynb) should be submitted with answers and code written in it. No separate file will be accepted.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer\n",
    "## Decision Making\n",
    "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
    "two parts. First, how do we learn about the world? This involves both the\n",
    "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
    "currently know about the world, how should we decide what to do, taking into\n",
    "account future events and observations that may change our conclusions?\n",
    "Typically, this will involve creating long-term plans covering possible future\n",
    "eventualities. That is, when planning under uncertainty, we also need to take\n",
    "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
    "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
    "known to produce good results and experiment with something new is known\n",
    "as the **exploration-exploitation dilemma**.\n",
    "\n",
    "## The exploration-exploitation trade-off\n",
    "\n",
    "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n",
    "best restaurant you have found so far was **Les Epinards**. The food there is\n",
    "usually to your taste and satisfactory. However, a well-known recommendations\n",
    "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
    "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
    "in which case you will regret going there. On the other hand, it could also be\n",
    "much better. What should you do?\n",
    "It all depends on how much information you have about either restaurant,\n",
    "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
    "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
    "Arm** to be significantly better. However, if you are going to stay there longer,\n",
    "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
    "better food for the remaining time, while otherwise you will have missed only\n",
    "one good meal out of many, making the potential risk quite small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
    "\n",
    "\n",
    "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n",
    "\n",
    "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
    "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
    "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
    "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n",
    "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n",
    "\n",
    "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n",
    "\n",
    "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_0 \\in S$, which is also known as the *value* for that state, \n",
    "\n",
    "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n",
    "\n",
    "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n",
    "\n",
    "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
    "\n",
    "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is\n",
    "\n",
    "$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n",
    "\n",
    "It turns out that a solution to the dynamic programming equation, known as the *Bellman equation*, is an optimal policy. The Bellman equation is given by\n",
    "\n",
    "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
    "\n",
    "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
    "\n",
    "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
    "\n",
    "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question covers a deterministic MPD, described as follows:\n",
    "\n",
    "* The agent starts in state **S**\n",
    "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
    "* The transition probabilities in each box are uniform. Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n",
    "* When reaching **F**, the game ends (absorbing state).\n",
    "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
    "* Assume no discount in this model: $\\gamma = 1$\n",
    "    \n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|-1 |1|**F**|\n",
    "|0|-1|1|  \n",
    "|-1 |0|-1|  \n",
    "|**S**|-1|1|\n",
    "\n",
    "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
    "\n",
    "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n",
    "\n",
    "Optimal path : EENNN. It gives the reward 0. \n",
    "\n",
    "It is not unique since for example the path EENNWNE also results in the reward 0.\n",
    "\n",
    "**1b)** What is the optimal policy (i.e. the optimal action in each state)?\n",
    "\n",
    "We want to receive positive rewards. Having this in mind, we reason about what the best action for each state is and we find that the following is an optimal policy. If we ever end up in the states next to the final state, we will enter the final state as other actions prove to be of less or equal total reward.\n",
    "\n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|E|E|**F**|\n",
    "|E|E|N|  \n",
    "|E|E|N|  \n",
    "|E|E|N|\n",
    "\n",
    "\n",
    "**1c)** What is expected total reward for the policy in 1b)?\n",
    " \n",
    " The expected total reward is 0, calculated with the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "policy = [[\"E\", \"E\", \"N\"], [\"E\", \"E\", \"N\"], [\"E\", \"E\", \"N\"], [\"E\", \"E\"], ]\n",
    "rewards = [[0, -1, 1], [-1, 0, -1], [0, -1, 1], [-1, 1, 0], ]\n",
    "\n",
    "def getReward(state):\n",
    "    return rewards[state[1]][state[0]]\n",
    "def getNewState(state, action):\n",
    "    return {\n",
    "        'N': (state[0], state[1]+1),\n",
    "        'W': (state[0]-1, state[1]),\n",
    "        'S': (state[0], state[1]-1),\n",
    "        'E': (state[0]+1, state[1]),\n",
    "    }[action]\n",
    "def getProbability(state, action, newState):\n",
    "     return 1 if getNewState(state, action) == newState else 0\n",
    "    \n",
    "sum = 0\n",
    "for i in range(0,4):\n",
    "    for j in range(0,3):\n",
    "        if i == 3 and j == 2:\n",
    "            break;\n",
    "        else:    \n",
    "            state = (j,i)\n",
    "            action = policy[i][j]\n",
    "            newState = getNewState(state, action)\n",
    "            sum = sum + getReward(newState)\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probabilit 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|10|0|  \n",
    "|0|0|0|  \n",
    "\n",
    "\n",
    "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|8|0|\n",
    "|8|2|8|  \n",
    "|0|8|0|  \n",
    "  \n",
    "**Iteration 2**:  \n",
    "  \n",
    "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
    "Action **S**: 0  \n",
    "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **W**: 0\n",
    "\n",
    "Hence any action between **E** and **N** would be best at this stage.\n",
    "\n",
    "Similarly for cell (1,0):\n",
    "\n",
    "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
    "\n",
    "Similar calculations for remaining cells give us:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|5.76|10.88|5.76|\n",
    "|10.88|8.12|10.88|  \n",
    "|5.76|10.88|5.76|  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop a function that given \n",
    "   - A dictionary of states and the possible actions at each state\n",
    "   - A function that given a state, an action and the current value function can calculate the expected value\n",
    "   \n",
    "will perform the value iteration and returns a converged value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "'''\n",
    "Performs a value iteration\n",
    "\n",
    "states should be a dictionary contain the states and the possible actions at those states\n",
    "\n",
    "calculateActionValue should be a function that given a state, an action and the current vCandidate,\n",
    "will calculate the action value. The vCandidate will be a dictionary with the states as the keys and \n",
    "the current values of those states as the values. \n",
    "\n",
    "Returns the converged value function. It will be a dictionary of the form {state:(best action(s), expected value)}\n",
    "'''\n",
    "def valueIteration(states, calculateActionValue):\n",
    "    #Create dictionary using the given states. Dictionary includes optimal action(s) and expected value for each state\n",
    "    vCandidate = {s:(None, 0) for s in states.keys()}\n",
    "    lastCandidate = {s:(None,1) for s in states.keys()}\n",
    "    \n",
    "    #Check if converging \n",
    "    while not np.allclose([value for _,(_,value) in vCandidate.items()], [value for _,(_,value) in lastCandidate.items()], atol=0.001):\n",
    "        lastCandidate = vCandidate.copy()\n",
    "        for state in states.keys():\n",
    "            #Calculate the action value for each possible action\n",
    "            actions = [(action, calculateActionValue(state, action, {s:value for  s, (_, value) in vCandidate.items()})) for action in states[state]]\n",
    "            #Calculate the maximum action value\n",
    "            maxActionValue = max([value for _,value in actions])\n",
    "            #Collect the best actions, that is, the actions with an action value equal to the max action value\n",
    "            bestActions = [(action, value) for action,value in actions if value == maxActionValue]\n",
    "            #Update the candidate with the max action value and the actions giving that value\n",
    "            vCandidate[state] = ([action for action,_ in bestActions], maxActionValue)\n",
    "\n",
    "    return vCandidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the state in our chosen datastructure and code the the function for calculating the action value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 Optimal action(s): ['N', 'E'] Expected value: 45.60639980464309\n",
      "State: 1 Optimal action(s): ['N'] Expected value: 51.94207337381977\n",
      "State: 2 Optimal action(s): ['N', 'W'] Expected value: 45.60744479398599\n",
      "State: 3 Optimal action(s): ['E'] Expected value: 51.94207337381977\n",
      "State: 4 Optimal action(s): ['N', 'E', 'S', 'W'] Expected value: 48.04646918422989\n",
      "State: 5 Optimal action(s): ['W'] Expected value: 51.94303101993309\n",
      "State: 6 Optimal action(s): ['S', 'E'] Expected value: 45.60744479398599\n",
      "State: 7 Optimal action(s): ['S'] Expected value: 51.94303101993309\n",
      "State: 8 Optimal action(s): ['S', 'W'] Expected value: 45.608322397269305\n"
     ]
    }
   ],
   "source": [
    "# The different states and the possible actions at each state\n",
    "state = {0:[\"N\", \"E\"], 1:[\"N\", \"E\", \"W\"], 2:[\"N\", \"W\"], 3:[\"N\", \"E\", \"S\"], 4:[\"N\", \"E\", \"S\", \"W\"], 5:[\"N\", \"W\", \"S\"], 6:[\"S\", \"E\"], 7:[\"S\", \"E\", \"W\"], 8:[\"S\", \"W\"]}\n",
    "\n",
    "#The rewards at the different states\n",
    "rewards = {0:0, 1:0, 2:0, 3:0, 4:10, 5:0, 6:0, 7:0, 8:0}\n",
    "\n",
    "#Util method for calculating new state given an action\n",
    "def getNewState(state, action):\n",
    "    if action == \"N\": return state + 3\n",
    "    if action == \"S\": return state - 3\n",
    "    if action == \"E\": return state + 1\n",
    "    if action == \"W\": return state - 1\n",
    "\n",
    "# Method for calculating the action value    \n",
    "def gridCalculateActionValue(state, action, vCandidate):\n",
    "    newState = getNewState(state, action)\n",
    "    return 0.8*(rewards[newState] + 0.9*vCandidate[newState]) + 0.2*(rewards[state] + 0.9*vCandidate[state])\n",
    "\n",
    "for state, (actions, values) in valueIteration(state, gridCalculateActionValue).items():\n",
    "    print(\"State:\",state, \"Optimal action(s):\",actions, \"Expected value:\", values)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the state dictionary and the action value function in our value iteration function and get the following:\n",
    "\n",
    "| | | |  \n",
    "|:----------:|:----------:|:---------:|  \n",
    "|['S', 'E'], 45.61100141518714|        ['S'], 51.94629036845465       |['S', 'W'], 45.611309320021036|\n",
    "|['E'], 51.94595438080162     |['N', 'E', 'S', 'W'], 48.05002580543105|['W'], 51.94629036845465      |  \n",
    "|['N', 'E'], 45.61063478338873|        ['N'], 51.94595438080162       |['N', 'W'], 45.61100141518714 |  \n",
    "\n",
    "The table above shows that in the states (1,0), (0,1), (2,1), (1,3), the best action is to take a step into the center cell. \n",
    "In all the other states, it do not matter which of the available actions you take. \n",
    "\n",
    "\n",
    "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n",
    "\n",
    "Since the value iteration is guaranteed to converge, it does not matter what $V_0$ we choose.\n",
    "The reason why the algorithm converges is because that we in each iteration performs the optimal action for each state. This means that we in each iteration will get a better approximation of the expected sum of rewards. \n",
    "\n",
    "Also, we think that the markov property is important to note. It says that (from wikipedia): \"A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it.\" \n",
    "\n",
    "So let us assume that our initial value instead is $V_2$, how did we get there? Oh by $V_1$ and $s_1$ ofcourse! How did we get to $V_1$? By $V_0$ and $s_0$. If it is the case that the value iteration converges, we can say that it is independent of the initial value $V_0$ because it in turn could be equal to any $V_t$ where $t\\in steps$. Using this logic, we can find a $V_t$ that precedes $V^*$, and then trace back to $V_{t-1}$ repeatedly (this is impossible to do backwards, so assume we have already converged), we will then be able to define a set of states and $V_t$ that converges. We think that this set is all states $s_t$ and $V_t$, and it makes the initial value $V_0$ not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning (RL)\n",
    "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
    "\n",
    "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "The value function and the action-value function are directly related through\n",
    "\n",
    "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
    "\n",
    "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
    "\n",
    "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
    "\n",
    "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n",
    "\n",
    "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n",
    "\n",
    "## OpenAI Gym\n",
    "\n",
    "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n",
    "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
    "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
    "\n",
    "## Question 3\n",
    "You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Take $\\gamma=0.95$. You can refer to the Q-learning Jupyter notebook shown in class, uploaded on Canvas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 Max diff:  4.487281485675467\n",
      "Iteration 2000 Max diff:  3.72187170280516\n",
      "Iteration 3000 Max diff:  0.3277704042915843\n",
      "Iteration 4000 Max diff:  0.24636154435009416\n",
      "Iteration 5000 Max diff:  0.16002478219117222\n",
      "Iteration 6000 Max diff:  0.017910499071945196\n",
      "Iteration 7000 Max diff:  0.01088275655611426\n",
      "\n",
      "******** Q solution ****************\n",
      "Q = \n",
      " [[61.41349497 60.59093387]\n",
      " [64.92959504 61.46849654]\n",
      " [69.54661594 62.56139748]\n",
      " [75.66481583 64.10335185]\n",
      " [83.59134547 66.0380737 ]]\n",
      "\n",
      "******** Policy ********************\n",
      "(forward,backward) = (0,1)\n",
      "\n",
      "[0 0 0 0 0] \n",
      "\n",
      "\n",
      "******** Optimal V from Q-learning *\n",
      "V = \n",
      " [61.41349497 64.92959504 69.54661594 75.66481583 83.59134547]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "env = gym.make('NChain-v0', slip=0.2)\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qcopy = np.ones((env.observation_space.n, env.action_space.n))\n",
    "#\n",
    "epsilon = 0.5\n",
    "# learning rate\n",
    "alfa = 0.1\n",
    "# discount factor\n",
    "gamma = 0.95\n",
    "\n",
    "iteration = 0\n",
    "# To make the algorithm converge we decay alfa\n",
    "alfadecay = 0.999\n",
    "while not np.allclose(Q,Qcopy, atol=0.001):\n",
    "    iteration += 1\n",
    "    old_state = env.reset()\n",
    "    alfa *= alfadecay\n",
    "    done = False\n",
    "    Qcopy = Q.copy() \n",
    "    while not done:\n",
    "        #Get the action to perform. Either we explore, or we exploit.\n",
    "        action = env.action_space.sample() if random.random() < epsilon else np.argmax(Q[old_state,:])\n",
    "        \n",
    "        #Perform the action and receive feedback\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # update Q value for old state following policy\n",
    "        Q[old_state][action] += alfa * \\\n",
    "            (reward + gamma * max(Q[new_state, :]) - Q[old_state][action])\n",
    "        \n",
    "        old_state = new_state\n",
    "\n",
    "    if(iteration % 1000 == 0):\n",
    "        diff = np.subtract(Q, Qcopy)\n",
    "        diff = np.absolute(diff)\n",
    "        print(\"Iteration\", iteration, \"Max diff: \", diff.max())\n",
    "\n",
    "\n",
    "print(\"\\n******** Q solution ****************\")\n",
    "print(\"Q = \\n\", Q)\n",
    "\n",
    "print(\"\\n******** Policy ********************\")\n",
    "print(\"(forward,backward) = (0,1)\\n\")\n",
    "\n",
    "print(np.argmax(Q,1), \"\\n\")\n",
    "\n",
    "print(\"\\n******** Optimal V from Q-learning *\")\n",
    "print(\"V = \\n\",np.max(Q,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. \n",
    "\n",
    "To verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action, we use our value iteration function from question 2 to calculate $V^*$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 Optimal action(s): [0] Expected value: 61.35402315043728\n",
      "State: 1 Optimal action(s): [0] Expected value: 64.86613478435599\n",
      "State: 2 Optimal action(s): [0] Expected value: 69.48693478435597\n",
      "State: 3 Optimal action(s): [0] Expected value: 75.56693478435596\n",
      "State: 4 Optimal action(s): [0] Expected value: 83.56693478435596\n"
     ]
    }
   ],
   "source": [
    "# Define the state and their possible actions as a dictionary\n",
    "state = {0:[0, 1], 1:[0, 1], 2:[0, 1], 3:[0, 1], 4:[0, 1]}\n",
    "# Define the rewards as a matrix that given a state and an action will return a reward\n",
    "rewards = [\n",
    "    [0, 2],\n",
    "    [0, 2],\n",
    "    [0, 2],\n",
    "    [0, 2],\n",
    "    [10, 2]\n",
    "]\n",
    "# Method for returning the new state given an action and a state \n",
    "def getNewState(state, action):\n",
    "    return {\n",
    "        0: state + 1 if state < 4 else state,\n",
    "        1: 0\n",
    "    }[action]\n",
    "# Method for calculating the expected value when performing an action\n",
    "def chainCalculateActionValue(state, action, vCandidate):\n",
    "    oppositeAction = abs(1-action)\n",
    "    newState = getNewState(state, action)\n",
    "    slipState = getNewState(state, oppositeAction)\n",
    "    return 0.8*(rewards[state][action] + 0.95*vCandidate[newState]) + 0.2*(rewards[state][oppositeAction] + 0.95*vCandidate[slipState])\n",
    "\n",
    "vq4 = valueIteration(state, chainCalculateActionValue)\n",
    "for state, (actions, values) in vq4.items():\n",
    "    print(\"State:\",state, \"Optimal action(s):\",actions, \"Expected value:\", values)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare whether $V^*$ is approximately equal to the optimal $Q^*$ value for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Q* and V* approximately equal? \u001b[1mTrue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Is Q* and V* approximately equal?\", \n",
    "      '\\033[1m' + str(np.allclose(np.amax(Q,1), [value for _,value in vq4.values()],  atol=0.5)) + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b)** What is the importance of exploration in RL? Explain with an example.\n",
    "\n",
    "In reinforcement learning the model recieves rewards for performing actions in different states. \n",
    "\n",
    "To learn the importance of exploration we must also learn the concept of exploatation. Exploatation can be seen as the opposite of exploration and if an agent only were to exploit, it could be simplified as a **greedy** algorithm. In the table above, exploatation means get reward for short term. Exploration means \"risking\" receiving no instant reward for the chance of receiving a larger reward in the future.\n",
    "\n",
    "Let us illustrate the concept with the entire chain-link example, the table shows the reward for performing actions forward or backward at link *n*.\n",
    "\n",
    "| link _n_ |     |     |     | forward | backward |\n",
    "| -------- | --- | --- | --- | ------- | -------- |\n",
    "| 0        |     |     |     | 0       | 2        |\n",
    "| 1        |     |     |     | 0       | 2        |\n",
    "| 2        |     |     |     | 0       | 2        |\n",
    "| 3        |     |     |     | 0       | 2        |\n",
    "| 4        |     |     |     | 0       | 2        |\n",
    "| 5        |     |     |     | 10      | 2        |\n",
    "\n",
    "The maximum reward (10) at one step _t_ is achievable by being at link _5_ and going forward. This reward greatly outperforms other available rewards and is always desirable. \n",
    "\n",
    "A simple example that shows the importance of exploration is shown in the figured below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RobertZetterlund/host-image/main/Draw_io_greedy.png\">\n",
    "\n",
    "\n",
    "In the example above, a model is tasked to find the best way from start to finish. If the model always exploits, that is, always take the best known action, it will never find out that there is a  way. Because it always exploits(always take the road to the right), it will not know all the possible states and hence miss out on a better reward.\n",
    "\n",
    "If the model however not only exploit but sometimes also explore it will, at some time, try the way to the left and then realise that the left way gives a better reward. It is however important to note that it is not a good idea to always explore and never exploit, as the model needs to learn from previous experiences. A model that only explores would in our example find the way to the left, but not learn that it is the best way and still sometimes choose the right road.\n",
    "\n",
    "To conclude, exploration is important since it helps to discover new states, possibly with bigger rewards. In more mathematical terms one could say that exploitation will always find a local optimum while a model that explores enough will ensure that the global optimum if found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
    "\n",
    "A decision tree is a diagram that is shaped like a tree. Simply put, it works by asking questions about the attributes of the given data and then classifying the data based on the answers. \n",
    "\n",
    "There are three important parts of the tree. These are:\n",
    "\n",
    " - Node: Asks a question about an attribute\n",
    " - Branch: Represents the answers of the question\n",
    " - Leaf: Is a class label\n",
    "\n",
    "When an instance is to be classified it starts at the root node. An attribute of the instance is tested, and the instance then moves down the branch corresponding to the result of the test. This process is repeated until the instance reaches a leaf. When that happens, the instance will be classified as the label the leaf holds. \n",
    "\n",
    "What attribute is investigated at what level is decided by how well a question split the data. The more uneven split a question creates, that is, the purer the subsets created are, the higher up in the tree.\n",
    "\n",
    "There are some drawbacks to decision trees, namely that they are overfitted to the dataset. For example, it is restricted to the labels that they know and performs poorly at new data. Data that is not very similar to the training set will be tough to accurately predict\n",
    "\n",
    "<!-- \n",
    "How the trees are built?\n",
    "How it is used for regression?\n",
    "--->\n",
    "\n",
    "To improve some drawbacks of decision trees it is possible to use random forests, which is an extension of the descision trees. Instead of using the complete dataset when building a tree, you only use a subset of the data, and on each level of the tree you use a random subset of the features of the dataset. And instead of only building one tree, several trees are built. \n",
    "\n",
    "It then works by letting each tree \"vote\" on which label the data should have, and the data is then classified with the label that got the most votes. \n",
    "\n",
    "**5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n",
    "\n",
    "The ability to learn by interacting with its environment is one aspect that makes reinforcement learning different from supervised learning tasks. In reinforcement learning, a model uses experiences it has received from past actions as input when choosing the next action, that is, the model **learns** from previous actions, via trial and error. This is different to supervised learning as in supervised learning the model is trained on labeled data. The model is initially given a training dataset, but will after this never change itself, that is, it will not **learn**. \n",
    "\n",
    "Another aspect that differs is how the model receives feedback. In reinforcement learning, the model receives rewards (or penalties) which indicates whether the action taken was a good or bad action from the environment (designed by the developer). In supervised learning on the other hand, feedback is based on how the developer evaluates the model after it has been trained.  \n",
    "\n",
    "Now, let us assume that we have created two regression models for predicting y given x, one using reinforcement and the other using supervised learning. A reward can be seen as accurately predicting a point and lets assume that the supervised model was using least-squares when trained. Given 10 new datapoints with the same x and y-values we can see that the supervised model will perform equally on all points, being already trained. The model using reinforcement learning will on the other hand perform better for each subsequent point (unless the model explores other options i.e `epsilon>0`) as it learns from experience. \n",
    "\n",
    "\n",
    "It can be argued that reinforcement learning is a type of supervised learning (bare with me), as the reward of the environment must be decided somehow. Assume a simple classification task and an unclassified datapoint: supervised nor reinforcement learning model can not learn unless they know the correct classification. The environment can not reward the agent with a reward unless the environment knows the quality and/or accuracy of the action performed by the agent.\n",
    "\n",
    "In summary, for tasks as regression and classification the two models are similar in the sense that they need labeled data (or environment). View the training phase for the supervised model as the same as the reinforcement agent learning the environment. The main difference is the exploratory nature of reinforcement learning. We would advise to not use reinforcement learning models for regression or classification because why learn from trial and error when the answers are available and it is possible to just apply supervised learning. For example, line regression least squares is optimized via supervised, so why try to reinvent it by updating an equation for a line based on experiences? As always, this depends on data being representative and the model being trained, tested and verified to a reasonable degree. An additional \"saving clause\" would be to mention that if the dataset for training is enourmous it might be hard to \"re-train\" the supervised model. We then assume that for simple regression line a case can be made for reinforcement learning as it is then relatively cheap to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "Primer/text based on the following references:\n",
    "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
    "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
